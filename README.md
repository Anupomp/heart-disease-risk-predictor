# Heart Disease Risk Predictor
## Data Management for Data Science - Complete Implementation

A comprehensive data science project that demonstrates advanced data management techniques, SQL-based analysis, ETL pipelines, and machine learning for heart disease prediction.

## üéØ Project Overview

This project implements a complete data management and analysis pipeline for heart disease prediction, specifically designed for **DATA MGMT FOR DATASC 01:198:210:G1** course requirements.

### Key Features
- **Database Design**: SQLite schema with proper constraints and indexing
- **SQL Analysis**: 10+ complex queries for exploratory data analysis
- **ETL Pipeline**: Complete extract-transform-load implementation
- **Machine Learning**: Multiple algorithms with hyperparameter tuning
- **Data Visualization**: Comprehensive statistical charts and EDA
- **Academic Documentation**: Complete project structure and analysis

## üìä Dataset

**Source**: Heart Disease UCI Dataset  
**Records**: 303 patients  
**Features**: 14 clinical attributes  
**Target**: Heart disease presence (binary classification)

### Features Description
- `age`: Age in years
- `sex`: Gender (0 = female, 1 = male)
- `cp`: Chest pain type (0-3)
- `trestbps`: Resting blood pressure
- `chol`: Serum cholesterol
- `fbs`: Fasting blood sugar > 120 mg/dl
- `restecg`: Resting ECG results
- `thalch`: Maximum heart rate achieved
- `exang`: Exercise induced angina
- `oldpeak`: ST depression induced by exercise
- `slope`: Slope of peak exercise ST segment
- `ca`: Number of major vessels colored by fluoroscopy
- `thal`: Thalassemia type
- `num`: Heart disease diagnosis (0-4, >0 indicates disease)

## üèóÔ∏è System Architecture

### 1. Database Layer (`database_setup.py`)
```
‚îú‚îÄ‚îÄ SQLite Database with proper schema
‚îú‚îÄ‚îÄ Data integrity constraints
‚îú‚îÄ‚îÄ Performance indexes
‚îú‚îÄ‚îÄ Automated data validation
‚îî‚îÄ‚îÄ Comprehensive table management
```

**Key Components:**
- **DatabaseManager Class**: Handles all database operations
- **Schema Design**: Proper data types, constraints, and relationships
- **Indexing Strategy**: Optimized for analytical queries
- **Data Integrity**: Automated validation and constraint checking

### 2. SQL Analysis Layer (`sql_analysis.py`)
```
‚îú‚îÄ‚îÄ 10+ Complex SQL Queries
‚îú‚îÄ‚îÄ Exploratory Data Analysis
‚îú‚îÄ‚îÄ Statistical Aggregations
‚îú‚îÄ‚îÄ Multi-factor Risk Analysis
‚îî‚îÄ‚îÄ Data Quality Assessment
```

**Analysis Categories:**
- Basic dataset statistics
- Target variable distribution
- Age group analysis
- Gender-based analysis
- Chest pain type correlation
- Cholesterol level impact
- Blood pressure analysis
- Exercise angina patterns
- High-risk patient identification
- Data quality verification

### 3. ETL Pipeline (`etl_pipeline.py`)
```
‚îú‚îÄ‚îÄ Extract: CSV data ingestion
‚îú‚îÄ‚îÄ Transform: Data cleaning & validation
‚îú‚îÄ‚îÄ Load: Database population
‚îú‚îÄ‚îÄ Analysis: SQL-based exploration
‚îî‚îÄ‚îÄ Reporting: Comprehensive logs
```

**Pipeline Features:**
- **Automated Processing**: Complete data flow automation
- **Error Handling**: Comprehensive exception management
- **Logging System**: Detailed operation tracking
- **Data Validation**: Multi-level quality checks
- **Performance Monitoring**: Processing metrics and statistics

### 4. Machine Learning Layer (`model_pipeline.py`)
```
‚îú‚îÄ‚îÄ Data Preprocessing
‚îú‚îÄ‚îÄ Feature Engineering
‚îú‚îÄ‚îÄ Model Training (Multiple Algorithms)
‚îú‚îÄ‚îÄ Hyperparameter Tuning
‚îî‚îÄ‚îÄ Performance Evaluation
```

**ML Components:**
- Random Forest Classifier
- Logistic Regression
- Gradient Boosting
- Cross-validation
- Feature importance analysis
- ROC curve analysis
- Confusion matrix evaluation

## üöÄ Getting Started

### Prerequisites
```bash
Python 3.8+
SQLite3
Git
```

### Installation
```bash
# Clone the repository
git clone https://github.com/AdityaBaranwal1/heart-disease-risk-predictor.git
cd heart-disease-risk-predictor

# Install dependencies
pip install -r requirements.txt
```

### Usage

#### 1. Run Complete ETL Pipeline
```bash
python etl_pipeline.py
```
This will:
- Extract data from CSV
- Transform and clean the data
- Load into SQLite database
- Execute comprehensive SQL analysis
- Generate detailed logs

#### 2. Database Setup (Standalone)
```bash
python database_setup.py
```

#### 3. SQL Analysis (Standalone)
```bash
python sql_analysis.py
```

#### 4. Machine Learning Pipeline
```bash
python model_pipeline.py
```

#### 5. Data Cleaning (Standalone)
```bash
python data_cleaning.py
```

## üìÅ Project Structure

```
heart-disease-risk-predictor/
‚îú‚îÄ‚îÄ üìä Data Management Components
‚îÇ   ‚îú‚îÄ‚îÄ database_setup.py      # SQLite database design & management
‚îÇ   ‚îú‚îÄ‚îÄ sql_analysis.py        # Comprehensive SQL analysis
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py        # Complete ETL implementation
‚îÇ   ‚îî‚îÄ‚îÄ data_cleaning.py       # Data preprocessing utilities
‚îú‚îÄ‚îÄ ü§ñ Machine Learning Components
‚îÇ   ‚îú‚îÄ‚îÄ model_pipeline.py      # ML model training & evaluation
‚îÇ   ‚îî‚îÄ‚îÄ model_training.ipynb   # Jupyter notebook for analysis
‚îú‚îÄ‚îÄ üìà Outputs & Results
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png   # Model evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.png # Feature analysis
‚îÇ   ‚îú‚îÄ‚îÄ roc_curve.png          # ROC curve analysis
‚îÇ   ‚îî‚îÄ‚îÄ model.pkl              # Trained model
‚îú‚îÄ‚îÄ üìã Data & Documentation
‚îÇ   ‚îú‚îÄ‚îÄ heart_disease_data.csv # Source dataset
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_report.md   # Model evaluation report
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.sql       # SQL scripts
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ üìù Configuration
    ‚îú‚îÄ‚îÄ README.md              # This file
    ‚îî‚îÄ‚îÄ .gitignore             # Git ignore rules
```

## üíæ Database Design

### Heart Disease Table Schema
```sql
CREATE TABLE heart_disease (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    age INTEGER NOT NULL CHECK (age > 0 AND age < 120),
    sex INTEGER NOT NULL CHECK (sex IN (0, 1)),
    cp INTEGER NOT NULL CHECK (cp >= 0 AND cp <= 3),
    trestbps INTEGER CHECK (trestbps > 0 AND trestbps < 300),
    chol INTEGER CHECK (chol >= 0),
    fbs INTEGER CHECK (fbs IN (0, 1)),
    restecg INTEGER CHECK (restecg >= 0 AND restecg <= 2),
    thalch INTEGER CHECK (thalch > 0 AND thalch < 250),
    exang TEXT CHECK (exang IN ('TRUE', 'FALSE')),
    oldpeak REAL CHECK (oldpeak >= 0),
    slope INTEGER CHECK (slope >= 0 AND slope <= 2),
    ca INTEGER CHECK (ca >= 0 AND ca <= 4),
    thal INTEGER CHECK (thal >= 0 AND thal <= 3),
    num INTEGER NOT NULL CHECK (num >= 0 AND num <= 4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Performance Indexes
- `idx_age`: Age-based queries
- `idx_sex`: Gender analysis
- `idx_num`: Target variable queries
- `idx_age_sex`: Combined demographic analysis
- `idx_chol`: Cholesterol-based analysis
- `idx_trestbps`: Blood pressure analysis

## üìä SQL Analysis Examples

### Age Group Risk Analysis
```sql
SELECT 
    CASE 
        WHEN age < 40 THEN 'Under 40'
        WHEN age BETWEEN 40 AND 50 THEN '40-50'
        WHEN age BETWEEN 51 AND 60 THEN '51-60'
        ELSE 'Over 60'
    END as age_group,
    COUNT(*) as total_patients,
    SUM(CASE WHEN num > 0 THEN 1 ELSE 0 END) as with_disease,
    ROUND(AVG(CASE WHEN num > 0 THEN 1.0 ELSE 0.0 END) * 100, 2) as disease_rate_percent
FROM heart_disease 
GROUP BY age_group 
ORDER BY MIN(age);
```

### Multi-Factor Risk Assessment
```sql
SELECT 
    CASE 
        WHEN age > 55 AND chol > 240 AND trestbps > 140 THEN '3 Risk Factors'
        WHEN (age > 55 AND chol > 240) OR (age > 55 AND trestbps > 140) OR (chol > 240 AND trestbps > 140) THEN '2 Risk Factors'
        WHEN age > 55 OR chol > 240 OR trestbps > 140 THEN '1 Risk Factor'
        ELSE 'No Major Risk Factors'
    END as risk_category,
    COUNT(*) as patient_count,
    SUM(CASE WHEN num > 0 THEN 1 ELSE 0 END) as with_disease,
    ROUND(AVG(CASE WHEN num > 0 THEN 1.0 ELSE 0.0 END) * 100, 2) as disease_rate_percent
FROM heart_disease 
GROUP BY risk_category
ORDER BY disease_rate_percent DESC;
```

## ü§ñ Machine Learning Results

### Model Performance
- **Random Forest**: 84% accuracy
- **Logistic Regression**: 86% accuracy  
- **Gradient Boosting**: 88% accuracy

### Key Features (by importance)
1. `thalch` (Maximum heart rate)
2. `oldpeak` (ST depression)
3. `cp` (Chest pain type)
4. `age` (Patient age)
5. `ca` (Major vessels)

## üìà Data Management Achievements

### Academic Requirements Fulfilled ‚úÖ
- [x] **Database Design**: Proper SQLite schema with constraints
- [x] **SQL Analysis**: 10+ complex analytical queries
- [x] **ETL Pipeline**: Complete automated data processing
- [x] **Data Quality**: Comprehensive validation and integrity checks
- [x] **Performance Optimization**: Strategic indexing for analytical workloads
- [x] **Documentation**: Complete academic-level documentation
- [x] **Logging & Monitoring**: Detailed operation tracking
- [x] **Error Handling**: Robust exception management
- [x] **Scalability**: Modular design for extensibility

### Data Management Best Practices
- **ACID Compliance**: Proper transaction management
- **Data Integrity**: Constraint-based validation
- **Query Optimization**: Index-based performance tuning
- **Separation of Concerns**: Modular architecture
- **Comprehensive Logging**: Audit trail for all operations
- **Error Recovery**: Graceful handling of edge cases

## üîß Technical Specifications

### Dependencies
- **pandas**: Data manipulation and analysis
- **numpy**: Numerical computing
- **scikit-learn**: Machine learning algorithms
- **matplotlib/seaborn**: Data visualization
- **sqlite3**: Database management
- **sqlalchemy**: Database ORM
- **jupyter**: Interactive development

### System Requirements
- **Python**: 3.8 or higher
- **Memory**: 4GB RAM minimum
- **Storage**: 100MB for datasets and models
- **OS**: Windows, macOS, or Linux

## üìù Academic Context

### Course: DATA MGMT FOR DATASC 01:198:210:G1
This project demonstrates mastery of:
- **Database Design Principles**
- **SQL Query Optimization**
- **ETL Pipeline Development**
- **Data Quality Management**
- **Performance Monitoring**
- **Academic Documentation Standards**

### Grading Criteria Addressed
- ‚úÖ Database schema design and implementation
- ‚úÖ Complex SQL query development
- ‚úÖ ETL pipeline architecture
- ‚úÖ Data integrity and validation
- ‚úÖ Performance optimization
- ‚úÖ Comprehensive documentation
- ‚úÖ Code quality and organization

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/enhancement`)
3. Commit changes (`git commit -am 'Add enhancement'`)
4. Push to branch (`git push origin feature/enhancement`)
5. Create Pull Request

## üìÑ License

This project is developed for academic purposes as part of the Data Management for Data Science course.

## üôè Acknowledgments

- UCI Machine Learning Repository for the heart disease dataset
- Course instructors for Data Management guidance
- Open source community for tools and libraries

---

**Note**: This implementation represents a comprehensive data management system suitable for academic evaluation and real-world application. All components are designed to demonstrate best practices in database design, SQL analysis, ETL development, and data science workflows.
